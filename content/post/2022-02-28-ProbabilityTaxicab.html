---
title: "What's Probability Mean? A Taxicab Forecasting Case"
author: "Alex Abraham"
date: 2023-02-28
categories: ["statistics", "bayesian", "frequentism"]
tags: ["statistics", "bayesian", "frequentism"]
---



<p>What’s the meaning of <em>probability</em>? There are two primary views. The “frequentist” view defines probability as event frequency over repeated trials – if a coin lands heads in 50 out of 100 flips, there’s 50% probability of heads. On the other hand, the Bayesian view defines probability as “strength of belief” in possible outcomes of one trial – how a single geopolitical event unfolds, for example.</p>
<p>From the two views of probability follow two statistical philosophies. Each philosophy involves (1) observed data and (2) hypotheses about the process which generates those data. <strong>The “frequentist” philosophy estimates probability of data after assuming a hypothesis</strong>–the hypothesis itself has no probability, as it lacks a repeated trial nature. <strong>The Bayesian philosophy estimates probability of both data and hypothesis</strong>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<p>We can see these two philosophies’ first principles with an example problem: how many taxicabs are in an unfamiliar city?</p>
<p>For more about the calculations that follow, check out my <a href="https://github.com/abrahamalex13/bayesian-case-studies/blob/main/taxicab/main.R">R code</a>.</p>
<div id="problem-statement" class="section level1">
<h1>Problem Statement</h1>
<p>“Suppose you arrive in a new city and see a taxi numbered 100. How many taxis are there in this city?”<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> Taxis are numbered 0, 1, 2, 3, …, up to the last taxi.</p>
</div>
<div id="a-classical-approach" class="section level1">
<h1>A Classical Approach</h1>
<p>We already have intuition about how to proceed. The likelihood of the observed data – taxicab #100 – depends on how many taxis there are in total. Our estimate of total taxi count should somehow elevate the likelihood of what we observed.</p>
<p>A classical approach: estimate a total taxi count which <em>maximizes</em> the likelihood of the observed data.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> Under this approach, we estimate 101 total taxis. (Numbering started at zero.) Why? When the city has 101 taxis, the likelihood of observing taxicab #100 equals 1/101. This probability decreases from its maximum if the taxi estimate rises above 101.</p>
</div>
<div id="classical-frequentist-solution-may-be-too-narrow" class="section level1">
<h1>Classical (Frequentist) Solution May Be Too Narrow</h1>
<p>The classical approach may be too narrow to address our true problem. Suppose we want to quantify uncertainty in our total taxi estimate (hypothesis), given the data we observe. The classical approach does not directly accommodate that calculation. The limitation traces back to the classical approach’s fundamental definition of uncertainty/probability: event frequency over repeated trials. Hence the methodology name, “frequentism”.</p>
<p>Classical estimation uncertainty quantifies probability of data–which we could repeatedly sample, in theory–given a hypothesis about the world. An analyst might say: “if this idea about the world were true, then I’m unlikely to observe these data or some still more extreme.”</p>
<p>Frequentists do not quantify probability of a hypothesis–a single event–given data. For this calculation, we must take a different approach.</p>
</div>
<div id="a-bayesian-approach" class="section level1">
<h1>A Bayesian Approach</h1>
<p>Suppose that we fundamentally define probability as,
“strength of belief” in possible outcomes for an event.
Now we can quantify the probability of a hypothesis, given data.
The procedure follows directly from a
fundamental statistical statement, Bayes’ Theorem:</p>
<ol style="list-style-type: decimal">
<li>Quantify prior belief over hypotheses in a complete set</li>
<li>Update beliefs: up-weighting hypotheses which make observed data more likely, down-weighting hypotheses which make observed data less likely</li>
</ol>
<p>We see this approach in action with the taxicab problem.</p>
<div id="quantify-prior-beliefs-over-hypotheses-for-total-taxi-count" class="section level2">
<h2>Quantify Prior Beliefs Over Hypotheses for Total Taxi Count</h2>
<p>First we assign a “strength of belief” weight to each
hypothesis for total taxi count.
For problem-solving convenience, we can generate weights from
the formula of a well-defined probability distribution.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p>Because our prior beliefs influence our answer,
it’s good practice to examine multiple scenarios of prior belief:</p>
<p><img src="/post/2022-02-28-ProbabilityTaxicab_files/figure-html/unnamed-chunk-1-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="update-beliefs-about-total-taxi-count" class="section level2">
<h2>Update Beliefs About Total Taxi Count</h2>
<p>Given the Taxi #100 evidence and our prior beliefs expression – the belief update is formulaic, and mathematically prescribed.</p>
<p>In each prior beliefs scenario,
we increase our strength of belief in hypotheses
around 101 total taxis. Because, when there are around 101 total taxis, the sighting of Taxi #100 is relatively more likely.</p>
<p>As expected, our hypotheses still reflect sizable uncertainty.
We’ve observed only one taxi, and the next sighting could suggest a very different total count. So we maintain positive weight on many hypotheses.</p>
<p><img src="/post/2022-02-28-ProbabilityTaxicab_files/figure-html/unnamed-chunk-2-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="forecasting-the-next-taxi" class="section level2">
<h2>Forecasting the Next Taxi</h2>
<p>We can forecast the next taxi sighting, using either set of beliefs about total taxi count (updated and prior). We’ll essentially average over our beliefs set.</p>
<p>Probability forecasts rise for next-taxi numbers made most likely under high-weight beliefs about total taxi count.</p>
<p><img src="/post/2022-02-28-ProbabilityTaxicab_files/figure-html/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>For more on how the philosophies assign probability to data and hypotheses, a helpful resource: <a href="https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/76926c6068c8b18e6ecb48d649b15bfd_MIT18_05S14_Reading20.pdf" class="uri">https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/76926c6068c8b18e6ecb48d649b15bfd_MIT18_05S14_Reading20.pdf</a>.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>I find this question in Kevin Murphy’s <em>Machine Learning: A Probabilistic Perspective</em>. A great read!<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>“Maximum Likelihood Estimation” is a fundamental method in statistics.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>In particular, the Pareto distribution.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
